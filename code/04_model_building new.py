"""
æ”¹è¿›çš„æ¨¡å‹å»ºç«‹æ¨¡å— - é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
"""

import pandas as pd
import numpy as np
import pickle
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import cross_val_score, RandomizedSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# å¯¼å…¥å¤šç§æ¨¡å‹
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier

def load_training_data():
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    from pathlib import Path
    
    project_root = Path(__file__).parent.parent
    processed_dir = project_root / "data" / "processed"
    
    train_path = processed_dir / "train_data.csv"
    test_path = processed_dir / "test_data.csv"
    feature_path = processed_dir / "feature_list.csv"
    
    if not (train_path.exists() and test_path.exists()):
        print("è¯·å…ˆè¿è¡Œ03_statistical_analysis.pyæˆ–02_data_preprocessing.py")
        return None, None, None, None, None, None, None
    
    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    
    if feature_path.exists():
        feature_df = pd.read_csv(feature_path)
        feature_cols = feature_df['feature'].tolist()
    else:
        # å¦‚æœæ²¡æœ‰ç‰¹å¾åˆ—è¡¨æ–‡ä»¶ï¼Œåˆ™è‡ªåŠ¨è¯†åˆ«ç‰¹å¾
        feature_cols = [col for col in train_df.columns 
                       if col not in ['HOSPITAL_EXPIRE_FLAG', 'SUBJECT_ID']]
    
    X_train = train_df[feature_cols]
    y_train = train_df['HOSPITAL_EXPIRE_FLAG']
    
    X_test = test_df[feature_cols]
    y_test = test_df['HOSPITAL_EXPIRE_FLAG']
    
    print(f"è®­ç»ƒé›†: {X_train.shape}")
    print(f"æµ‹è¯•é›†: {X_test.shape}")
    print(f"ç‰¹å¾æ•°: {len(feature_cols)}")
    
    # è¯¦ç»†åˆ†æç±»åˆ«åˆ†å¸ƒ
    train_pos = sum(y_train == 1)
    train_neg = sum(y_train == 0)
    test_pos = sum(y_test == 1)
    test_neg = sum(y_test == 0)
    
    print(f"\nè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
    print(f"  é˜´æ€§(0): {train_neg} ({train_neg/len(y_train):.2%})")
    print(f"  é˜³æ€§(1): {train_pos} ({train_pos/len(y_train):.2%})")
    print(f"  ä¸å¹³è¡¡æ¯”ä¾‹: {train_neg/train_pos:.2f}:1")
    
    print(f"\næµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ:")
    print(f"  é˜´æ€§(0): {test_neg} ({test_neg/len(y_test):.2%})")
    print(f"  é˜³æ€§(1): {test_pos} ({test_pos/len(y_test):.2%})")
    
    return X_train, y_train, X_test, y_test, feature_cols, train_df, test_df

def scale_features(X_train, X_test):
    """ç‰¹å¾æ ‡å‡†åŒ–"""
    print("\nç‰¹å¾æ ‡å‡†åŒ–...")
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ä¿å­˜scaler
    from pathlib import Path
    models_dir = Path(__file__).parent.parent / "outputs" / "models"
    models_dir.mkdir(parents=True, exist_ok=True)
    
    with open(models_dir / 'scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    print("ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆï¼Œscalerå·²ä¿å­˜")
    
    return X_train_scaled, X_test_scaled

def handle_class_imbalance(X_train, y_train, method='smote'):
    """å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜"""
    print(f"\nå¤„ç†ç±»åˆ«ä¸å¹³è¡¡... (æ–¹æ³•: {method})")
    
    if method == 'smote':
        # ä½¿ç”¨SMOTEè¿‡é‡‡æ ·
        smote = SMOTE(random_state=42, k_neighbors=min(5, sum(y_train == 1) - 1))
        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
        
        print(f"è¿‡é‡‡æ ·å‰: 0={sum(y_train==0)}, 1={sum(y_train==1)}")
        print(f"è¿‡é‡‡æ ·å: 0={sum(y_resampled==0)}, 1={sum(y_resampled==1)}")
        
        return X_resampled, y_resampled
    
    elif method == 'class_weight':
        # è¿”å›åŸå§‹æ•°æ®ï¼Œä½†åœ¨æ¨¡å‹ä¸­è®¾ç½®class_weight
        print("ä½¿ç”¨class_weightå‚æ•°å¤„ç†ä¸å¹³è¡¡")
        return X_train, y_train
    
    return X_train, y_train

def define_models_with_imbalance_handling():
    """å®šä¹‰è¦è®­ç»ƒçš„æ¨¡å‹ - é’ˆå¯¹ä¸å¹³è¡¡æ•°æ®ä¼˜åŒ–"""
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    # åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œéœ€è¦æ ¹æ®å®é™…æ•°æ®è®¡ç®—
    
    models = {
        'Logistic_Regression': LogisticRegression(
            random_state=42, 
            max_iter=1000,
            class_weight='balanced',  # è‡ªåŠ¨å¹³è¡¡ç±»åˆ«æƒé‡
            solver='liblinear'
        ),
        'Random_Forest': RandomForestClassifier(
            random_state=42, 
            n_jobs=-1,
            class_weight='balanced_subsample',  # å¤„ç†ä¸å¹³è¡¡
            n_estimators=200,  # å¢åŠ æ ‘çš„æ•°é‡
            min_samples_split=10,  # é˜²æ­¢è¿‡æ‹Ÿåˆ
            min_samples_leaf=5
        ),
        'XGBoost': XGBClassifier(
            random_state=42, 
            eval_metric='logloss',
            use_label_encoder=False,
            n_jobs=-1,
            scale_pos_weight=10,  # å¢åŠ æ­£æ ·æœ¬æƒé‡ï¼Œå€¼éœ€è¦æ ¹æ®ä¸å¹³è¡¡æ¯”ä¾‹è°ƒæ•´
            max_depth=5,  # é™åˆ¶æ·±åº¦é˜²æ­¢è¿‡æ‹Ÿåˆ
            subsample=0.8,
            colsample_bytree=0.8
        ),
        'LightGBM': LGBMClassifier(
            random_state=42, 
            n_jobs=-1,
            verbose=-1,
            is_unbalance=True,  # å¤„ç†ä¸å¹³è¡¡
            boosting_type='gbdt',
            num_leaves=31,
            max_depth=5,
            min_child_samples=20
        ),
        'Gradient_Boosting': GradientBoostingClassifier(
            random_state=42,
            n_estimators=200,
            learning_rate=0.05,  # é™ä½å­¦ä¹ ç‡
            max_depth=5,
            min_samples_split=10,
            min_samples_leaf=5
        ),
        'Neural_Network': MLPClassifier(
            random_state=42,
            max_iter=500,
            early_stopping=True,
            hidden_layer_sizes=(100, 50),
            alpha=0.01,
            learning_rate='adaptive'
        )
    }
    
    print(f"å°†è®­ç»ƒ {len(models)} ç§æ¨¡å‹:")
    for name, model in models.items():
        print(f"  - {name}")
    
    return models

def evaluate_model_with_threshold(model, X_train, y_train, X_test, y_test, model_name):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹ - ä½¿ç”¨é˜ˆå€¼è°ƒæ•´"""
    from sklearn.metrics import classification_report, confusion_matrix, roc_curve
    
    # è®­ç»ƒæ¨¡å‹
    print(f"  è®­ç»ƒ{model_name}...")
    model.fit(X_train, y_train)
    
    # é¢„æµ‹æ¦‚ç‡
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # å¯»æ‰¾æœ€ä½³é˜ˆå€¼
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
    
    # ä½¿ç”¨Youden's Jç»Ÿè®¡é‡æ‰¾åˆ°æœ€ä½³é˜ˆå€¼
    j_scores = tpr - fpr
    best_threshold = thresholds[np.argmax(j_scores)]
    
    print(f"    æœ€ä½³é˜ˆå€¼: {best_threshold:.4f}")
    
    # ä½¿ç”¨æœ€ä½³é˜ˆå€¼è¿›è¡Œé¢„æµ‹
    y_pred = (y_pred_proba >= best_threshold).astype(int)
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    metrics = {
        'å‡†ç¡®ç‡': accuracy_score(y_test, y_pred),
        'ç²¾ç¡®ç‡': precision_score(y_test, y_pred, zero_division=0),
        'å¬å›ç‡': recall_score(y_test, y_pred, zero_division=0),
        'F1åˆ†æ•°': f1_score(y_test, y_pred, zero_division=0),
        'AUC': roc_auc_score(y_test, y_pred_proba),
        'æœ€ä½³é˜ˆå€¼': best_threshold
    }
    
    # äº¤å‰éªŒè¯ï¼ˆä½¿ç”¨AUCä½œä¸ºè¯„åˆ†ï¼‰
    try:
        cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='roc_auc', n_jobs=-1)
        metrics['äº¤å‰éªŒè¯AUCå‡å€¼'] = cv_scores.mean()
        metrics['äº¤å‰éªŒè¯AUCæ ‡å‡†å·®'] = cv_scores.std()
    except:
        metrics['äº¤å‰éªŒè¯AUCå‡å€¼'] = np.nan
        metrics['äº¤å‰éªŒè¯AUCæ ‡å‡†å·®'] = np.nan
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print("    åˆ†ç±»æŠ¥å‘Š:")
    report = classification_report(y_test, y_pred, target_names=['Alive', 'Death'], digits=4)
    for line in report.split('\n'):
        print(f"      {line}")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_test, y_pred)
    print(f"    æ··æ·†çŸ©é˜µ:\n{cm}")
    
    return model, metrics, best_threshold

def train_and_evaluate_all_models_improved(X_train, y_train, X_test, y_test):
    """è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰æ¨¡å‹ - æ”¹è¿›ç‰ˆæœ¬"""
    print("\n" + "="*60)
    print("è®­ç»ƒå’Œè¯„ä¼°å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹ (æ”¹è¿›ç‰ˆ)")
    print("="*60)
    
    models = define_models_with_imbalance_handling()
    results = {}
    trained_models = {}
    thresholds = {}
    
    for name, model in models.items():
        print(f"\nå¤„ç†æ¨¡å‹: {name}")
        try:
            # å¤„ç†ä¸å¹³è¡¡ï¼ˆå¯é€‰ï¼Œå¯ä»¥åœ¨æ¨¡å‹å‚æ•°ä¸­å¤„ç†ï¼‰
            if name in ['Logistic_Regression', 'Random_Forest', 'LightGBM']:
                # è¿™äº›æ¨¡å‹å·²ç»æœ‰å†…ç½®çš„ä¸å¹³è¡¡å¤„ç†
                X_train_balanced, y_train_balanced = X_train, y_train
            else:
                # å¯¹æ²¡æœ‰å†…ç½®å¤„ç†çš„æ¨¡å‹ä½¿ç”¨SMOTE
                X_train_balanced, y_train_balanced = handle_class_imbalance(X_train, y_train, method='smote')
            
            trained_model, metrics, threshold = evaluate_model_with_threshold(
                model, X_train_balanced, y_train_balanced, X_test, y_test, name
            )
            
            results[name] = metrics
            trained_models[name] = trained_model
            thresholds[name] = threshold
            
            print(f"  æ€§èƒ½æŒ‡æ ‡:")
            for metric_name, value in metrics.items():
                if not pd.isna(value) and metric_name not in ['æœ€ä½³é˜ˆå€¼', 'åˆ†ç±»æŠ¥å‘Š']:
                    print(f"    {metric_name}: {value:.4f}")
                    
        except Exception as e:
            print(f"   è®­ç»ƒ{name}æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            results[name] = None
    
    return results, trained_models, thresholds

def hyperparameter_tuning_focused(X_train, y_train, focus_models=None):
    """å¯¹å…³é”®æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§è°ƒä¼˜"""
    print("\n" + "="*60)
    print("å¯¹å…³é”®æ¨¡å‹è¿›è¡Œé’ˆå¯¹æ€§è¶…å‚æ•°è°ƒä¼˜")
    print("="*60)
    
    if focus_models is None:
        focus_models = ['Random_Forest', 'XGBoost', 'LightGBM']
    
    # å®šä¹‰é’ˆå¯¹æ€§å‚æ•°ç½‘æ ¼
    param_grids = {
        'Random_Forest': {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, None],
            'min_samples_split': [5, 10, 20],
            'min_samples_leaf': [2, 4, 8],
            'class_weight': ['balanced', 'balanced_subsample', {0: 1, 1: 3}]
        },
        'XGBoost': {
            'n_estimators': [100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.05, 0.1],
            'subsample': [0.6, 0.8, 1.0],
            'colsample_bytree': [0.6, 0.8, 1.0],
            'scale_pos_weight': [5, 10, 20]  # æ­£æ ·æœ¬æƒé‡
        },
        'LightGBM': {
            'n_estimators': [100, 200],
            'max_depth': [5, 10, 15],
            'learning_rate': [0.01, 0.05, 0.1],
            'num_leaves': [31, 50, 100],
            'subsample': [0.6, 0.8, 1.0],
            'is_unbalance': [True],
            'min_child_samples': [10, 20, 30]
        }
    }
    
    models_to_tune = {
        'Random_Forest': RandomForestClassifier(random_state=42, n_jobs=-1),
        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False, n_jobs=-1),
        'LightGBM': LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1)
    }
    
    tuned_models = {}
    tuning_results = {}
    
    for name in focus_models:
        if name not in models_to_tune:
            continue
            
        print(f"\nğŸ” å¯¹{name}è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
        
        try:
            # ä½¿ç”¨åˆ†å±‚äº¤å‰éªŒè¯
            cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            
            random_search = RandomizedSearchCV(
                models_to_tune[name], 
                param_grids[name], 
                n_iter=15,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                cv=cv,
                scoring='roc_auc',  # ä½¿ç”¨AUCä½œä¸ºè¯„åˆ†
                random_state=42,
                n_jobs=-1,
                verbose=1  # æ˜¾ç¤ºè¿›åº¦
            )
            
            random_search.fit(X_train, y_train)
            
            tuned_models[name] = random_search.best_estimator_
            tuning_results[name] = {
                'æœ€ä½³å‚æ•°': random_search.best_params_,
                'æœ€ä½³AUCåˆ†æ•°': random_search.best_score_
            }
            
            print(f"  æœ€ä½³å‚æ•°: {random_search.best_params_}")
            print(f"  æœ€ä½³äº¤å‰éªŒè¯AUCåˆ†æ•°: {random_search.best_score_:.4f}")
            
            # è¯„ä¼°è°ƒä¼˜åçš„æ¨¡å‹
            y_pred_proba = random_search.best_estimator_.predict_proba(X_train)[:, 1]
            auc_score = roc_auc_score(y_train, y_pred_proba)
            print(f"  è®­ç»ƒé›†AUC: {auc_score:.4f}")
            
        except Exception as e:
            print(f"  {name}è°ƒä¼˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    return tuned_models, tuning_results

def save_models_and_results_compatible(trained_models, results, tuned_models, tuning_results, feature_cols, thresholds):
    """ä¿å­˜æ¨¡å‹å’Œç»“æœ - ä¸ç¬¬äº”æ­¥ä»£ç å…¼å®¹"""
    from pathlib import Path
    import json
    
    # åˆ›å»ºç›®å½•
    models_dir = Path(__file__).parent.parent / "outputs" / "models"
    tables_dir = Path(__file__).parent.parent / "outputs" / "tables"
    
    models_dir.mkdir(parents=True, exist_ok=True)
    tables_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¨¡å‹åç§°æ˜ å°„
    model_name_mapping = {
        'Logistic_Regression': 'logistic_regression',
        'Random_Forest': 'random_forest',
        'XGBoost': 'xgboost',
        'LightGBM': 'lightgbm',
        'Gradient_Boosting': 'gradient_boosting',
        'Neural_Network': 'neural_network'
    }
    
    # 1. ä¿å­˜æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹ï¼‰
    print("\nä¿å­˜åŸºç¡€æ¨¡å‹...")
    for name, model in trained_models.items():
        try:
            if name in model_name_mapping:
                model_filename = model_name_mapping[name]
                model_path = models_dir / f'{model_filename}.pkl'
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
                print(f"  {name}: å·²ä¿å­˜ä¸º {model_filename}.pkl")
        except Exception as e:
            print(f"  ä¿å­˜{name}å¤±è´¥: {e}")
    
    # 2. ä¿å­˜è°ƒä¼˜åçš„æ¨¡å‹
    print("\nä¿å­˜è°ƒä¼˜åçš„æ¨¡å‹...")
    for name, model in tuned_models.items():
        try:
            if name in model_name_mapping:
                model_filename = model_name_mapping[name] + '_tuned'
                model_path = models_dir / f'{model_filename}.pkl'
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
                print(f"  {name}(è°ƒä¼˜å): å·²ä¿å­˜ä¸º {model_filename}.pkl")
        except Exception as e:
            print(f"  ä¿å­˜{name}(è°ƒä¼˜å)å¤±è´¥: {e}")
    
    # 3. ä¿å­˜é˜ˆå€¼ä¿¡æ¯
    threshold_df = pd.DataFrame(list(thresholds.items()), columns=['Model', 'Best_Threshold'])
    threshold_df.to_csv(tables_dir / 'model_best_thresholds.csv', index=False)
    print(f"  æ¨¡å‹æœ€ä½³é˜ˆå€¼å·²ä¿å­˜")
    
    # 4. ä¿å­˜ç»“æœåˆ°CSV
    print("\nä¿å­˜ç»“æœ...")
    
    # åŸºç¡€æ¨¡å‹ç»“æœ
    results_df = pd.DataFrame(results).T
    results_df = results_df.sort_values('AUC', ascending=False)
    results_df.to_csv(tables_dir / 'model_performance_comparison.csv')
    print(f"  âœ… æ¨¡å‹æ€§èƒ½æ¯”è¾ƒè¡¨å·²ä¿å­˜")
    
    # è°ƒä¼˜ç»“æœ
    if tuning_results:
        tuning_df = pd.DataFrame(tuning_results).T
        tuning_df.to_csv(tables_dir / 'hyperparameter_tuning_results.csv')
        print(f"  è¶…å‚æ•°è°ƒä¼˜ç»“æœå·²ä¿å­˜")
    
    # 5. ä¿å­˜ç‰¹å¾åˆ—è¡¨
    feature_df = pd.DataFrame({'feature': feature_cols})
    feature_df.to_csv(tables_dir / 'feature_list.csv', index=False)
    print(f"  ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜")
    
    # 6. ä¿å­˜ç»“æœä¸ºJSON
    results_dict = {
        'åŸºç¡€æ¨¡å‹æ€§èƒ½': results_df.to_dict(),
        'è¶…å‚æ•°è°ƒä¼˜ç»“æœ': tuning_results,
        'æœ€ä½³é˜ˆå€¼': thresholds,
        'ç‰¹å¾æ•°é‡': len(feature_cols)
    }
    
    with open(tables_dir / 'model_results.json', 'w') as f:
        json.dump(results_dict, f, indent=2, default=str)
    
    print(f"  å®Œæ•´ç»“æœå·²ä¿å­˜ä¸ºJSONæ ¼å¼")
    
    return results_df

def main_model_building_improved():
    """ä¸»æ¨¡å‹å»ºç«‹æµç¨‹ - æ”¹è¿›ç‰ˆæœ¬"""
    print("="*60)
    print("ICUæ­»äº¡ç‡é¢„æµ‹æ¨¡å‹å»ºç«‹ (æ”¹è¿›ç‰ˆ)")
    print("="*60)
    
    # 1. åŠ è½½æ•°æ®
    print("\næ­¥éª¤1: åŠ è½½æ•°æ®")
    data = load_training_data()
    if data[0] is None:
        return None, None, None
    
    X_train, y_train, X_test, y_test, feature_cols, train_df, test_df = data
    
    # 2. ç‰¹å¾å·¥ç¨‹ï¼ˆå¯é€‰ï¼Œæ ¹æ®éœ€è¦è¿›è¡Œï¼‰
    print("\næ­¥éª¤2: ç‰¹å¾å·¥ç¨‹")
    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ç‰¹å¾é€‰æ‹©ã€ç‰¹å¾åˆ›å»ºç­‰æ­¥éª¤
    
    # 3. ç‰¹å¾æ ‡å‡†åŒ–
    print("\næ­¥éª¤3: ç‰¹å¾æ ‡å‡†åŒ–")
    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
    
    # 4. è®­ç»ƒå’Œè¯„ä¼°å¤šç§æ¨¡å‹ï¼ˆåŸºç¡€æ¨¡å‹ï¼‰
    print("\næ­¥éª¤4: è®­ç»ƒå¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹")
    results, trained_models, thresholds = train_and_evaluate_all_models_improved(
        X_train_scaled, y_train, X_test_scaled, y_test
    )
    
    # 5. è¶…å‚æ•°è°ƒä¼˜
    print("\næ­¥éª¤5: è¶…å‚æ•°è°ƒä¼˜")
    
    # é€‰æ‹©è¡¨ç°è¾ƒå¥½çš„æ¨¡å‹è¿›è¡Œè°ƒä¼˜
    if results:
        results_df_pre = pd.DataFrame(results).T
        # é€‰æ‹©AUC > 0.7çš„æ¨¡å‹è¿›è¡Œè°ƒä¼˜
        good_models = results_df_pre[results_df_pre['AUC'] > 0.7].index.tolist()
        print(f"å°†å¯¹ä»¥ä¸‹æ¨¡å‹è¿›è¡Œè°ƒä¼˜: {good_models}")
        
        tuned_models, tuning_results = hyperparameter_tuning_focused(
            X_train_scaled, y_train, focus_models=good_models
        )
    else:
        tuned_models, tuning_results = {}, {}
    
    # 6. ä¿å­˜æ¨¡å‹å’Œç»“æœ
    print("\næ­¥éª¤6: ä¿å­˜æ¨¡å‹å’Œç»“æœ")
    results_df = save_models_and_results_compatible(
        trained_models, results, tuned_models, tuning_results, feature_cols, thresholds
    )
    
    # 7. åˆ†æç»“æœ
    print("\n" + "="*60)
    print("æ¨¡å‹å»ºç«‹å®Œæˆï¼")
    print("="*60)
    
    if not results_df.empty:
        print(f"\næ¨¡å‹æ€§èƒ½æ€»ç»“:")
        display_cols = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'AUC', 'æœ€ä½³é˜ˆå€¼']
        display_cols = [col for col in display_cols if col in results_df.columns]
        print(results_df[display_cols].head().to_string())
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹ï¼ˆåŸºäºF1åˆ†æ•°ï¼Œæ›´ç»¼åˆï¼‰
        if 'F1åˆ†æ•°' in results_df.columns:
            best_model_name = results_df['F1åˆ†æ•°'].idxmax()
            best_f1 = results_df.loc[best_model_name, 'F1åˆ†æ•°']
            best_auc = results_df.loc[best_model_name, 'AUC']
            print(f"\næœ€ä½³æ¨¡å‹ (åŸºäºF1åˆ†æ•°): {best_model_name}")
            print(f"  F1åˆ†æ•°: {best_f1:.4f}")
            print(f"  AUC: {best_auc:.4f}")
    
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"  1. æŸ¥çœ‹ outputs/tables/model_performance_comparison.csv")
    print(f"  2. æŸ¥çœ‹ outputs/tables/model_best_thresholds.csv")
    print(f"  3. è¿è¡Œ 05_model_evaluation.py è¿›è¡Œè¯¦ç»†è¯„ä¼°å’Œå¯è§†åŒ–")
    
    return results_df

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    results_df = main_model_building_improved()