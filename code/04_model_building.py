
import pandas as pd
import numpy as np
import pickle
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# å¯¼å…¥å¤šç§æ¨¡å‹
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

def load_training_data():
    """åŠ è½½è®­ç»ƒæ•°æ®"""
    from pathlib import Path
    
    project_root = Path(__file__).parent.parent
    processed_dir = project_root / "data" / "processed"
    
    train_path = processed_dir / "train_data.csv"
    test_path = processed_dir / "test_data.csv"
    
    if not (train_path.exists() and test_path.exists()):
        print("è¯·å…ˆè¿è¡Œ03_statistical_analysis.pyæˆ–02_data_preprocessing.py")
        return None, None, None, None
    
    # åŠ è½½æ•°æ®
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    feature_cols = [col for col in train_df.columns if col not in ['HOSPITAL_EXPIRE_FLAG', 'SUBJECT_ID']]
    
    X_train = train_df[feature_cols]
    y_train = train_df['HOSPITAL_EXPIRE_FLAG']
    
    X_test = test_df[feature_cols]
    y_test = test_df['HOSPITAL_EXPIRE_FLAG']
    
    print(f"è®­ç»ƒé›†: {X_train.shape}")
    print(f"æµ‹è¯•é›†: {X_test.shape}")
    print(f"ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"è®­ç»ƒé›†ç›®æ ‡åˆ†å¸ƒ: 0={sum(y_train==0)} ({sum(y_train==0)/len(y_train):.1%}), "
          f"1={sum(y_train==1)} ({sum(y_train==1)/len(y_train):.1%})")
    
    return X_train, y_train, X_test, y_test, feature_cols

def scale_features(X_train, X_test):
    """ç‰¹å¾æ ‡å‡†åŒ–"""
    print("\n ç‰¹å¾æ ‡å‡†åŒ–...")
    
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ä¿å­˜scaler
    from pathlib import Path
    models_dir = Path(__file__).parent.parent / "outputs" / "models"
    models_dir.mkdir(parents=True, exist_ok=True)
    
    with open(models_dir / 'scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    print(" ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆï¼Œscalerå·²ä¿å­˜")
    
    return X_train_scaled, X_test_scaled

def define_models():
    """å®šä¹‰è¦è®­ç»ƒçš„æ¨¡å‹"""
    models = {
        'é€»è¾‘å›å½’': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),
        'éšæœºæ£®æ—': RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced'),
        'æ¢¯åº¦æå‡': GradientBoostingClassifier(random_state=42),
        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),
        'LightGBM': LGBMClassifier(random_state=42, n_jobs=-1),
        'æ”¯æŒå‘é‡æœº': SVC(random_state=42, probability=True, class_weight='balanced'),
        'Kè¿‘é‚»': KNeighborsClassifier(n_jobs=-1),
        'æœ´ç´ è´å¶æ–¯': GaussianNB()
    }
    
    print(f"å°†è®­ç»ƒ {len(models)} ç§æ¨¡å‹:")
    for name, model in models.items():
        print(f"  - {name}")
    
    return models

def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
    import matplotlib.pyplot as plt
    
    # è®­ç»ƒæ¨¡å‹
    print(f"  è®­ç»ƒ{model_name}...")
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
    
    # è®¡ç®—å„é¡¹æŒ‡æ ‡
    metrics = {
        'å‡†ç¡®ç‡': accuracy_score(y_test, y_pred),
        'ç²¾ç¡®ç‡': precision_score(y_test, y_pred, zero_division=0),
        'å¬å›ç‡': recall_score(y_test, y_pred, zero_division=0),
        'F1åˆ†æ•°': f1_score(y_test, y_pred, zero_division=0)
    }
    
    if y_pred_proba is not None:
        metrics['AUC'] = roc_auc_score(y_test, y_pred_proba)
    
    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1', n_jobs=-1)
    metrics['äº¤å‰éªŒè¯F1å‡å€¼'] = cv_scores.mean()
    metrics['äº¤å‰éªŒè¯F1æ ‡å‡†å·®'] = cv_scores.std()
    
    return model, metrics

def train_and_evaluate_all_models(X_train, y_train, X_test, y_test):
    """è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
    print("\n" + "="*60)
    print(" è®­ç»ƒå’Œè¯„ä¼°å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹")
    print("="*60)
    
    models = define_models()
    results = {}
    trained_models = {}
    
    for name, model in models.items():
        print(f"\n å¤„ç†æ¨¡å‹: {name}")
        try:
            trained_model, metrics = evaluate_model(model, X_train, y_train, X_test, y_test, name)
            results[name] = metrics
            trained_models[name] = trained_model
            
            print(f"  æ€§èƒ½æŒ‡æ ‡:")
            for metric_name, value in metrics.items():
                print(f"    {metric_name}: {value:.4f}")
                
        except Exception as e:
            print(f"   è®­ç»ƒ{name}æ—¶å‡ºé”™: {e}")
            results[name] = None
    
    return results, trained_models

def hyperparameter_tuning(X_train, y_train):
    """å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜"""
    print("\n" + "="*60)
    print(" å¯¹æœ€ä½³æ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜")
    print("="*60)
    
    # å®šä¹‰å‚æ•°ç½‘æ ¼
    param_grids = {
        'éšæœºæ£®æ—': {
            'n_estimators': [50, 100, 200],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'class_weight': ['balanced', 'balanced_subsample']
        },
        'XGBoost': {
            'n_estimators': [50, 100, 200],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1, 0.3],
            'subsample': [0.8, 1.0],
            'colsample_bytree': [0.8, 1.0]
        },
        'LightGBM': {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 20],
            'learning_rate': [0.01, 0.1, 0.3],
            'num_leaves': [31, 50, 100],
            'subsample': [0.8, 1.0]
        }
    }
    
    # åªè°ƒä¼˜å‡ ä¸ªå…³é”®æ¨¡å‹
    models_to_tune = {
        'éšæœºæ£®æ—': RandomForestClassifier(random_state=42, n_jobs=-1),
        'XGBoost': XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),
        'LightGBM': LGBMClassifier(random_state=42, n_jobs=-1)
    }
    
    tuned_models = {}
    tuning_results = {}
    
    for name, model in models_to_tune.items():
        print(f"\nğŸ” å¯¹{name}è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜...")
        
        try:
            # ä½¿ç”¨éšæœºæœç´¢ï¼ˆæ¯”ç½‘æ ¼æœç´¢æ›´å¿«ï¼‰
            random_search = RandomizedSearchCV(
                model, param_grids[name], 
                n_iter=20,  # éšæœºå°è¯•20ç»„å‚æ•°
                cv=3, 
                scoring='f1',
                random_state=42,
                n_jobs=-1,
                verbose=0
            )
            
            random_search.fit(X_train, y_train)
            
            tuned_models[name] = random_search.best_estimator_
            tuning_results[name] = {
                'æœ€ä½³å‚æ•°': random_search.best_params_,
                'æœ€ä½³åˆ†æ•°': random_search.best_score_
            }
            
            print(f"  æœ€ä½³å‚æ•°: {random_search.best_params_}")
            print(f"  æœ€ä½³äº¤å‰éªŒè¯F1åˆ†æ•°: {random_search.best_score_:.4f}")
            
        except Exception as e:
            print(f"   {name}è°ƒä¼˜å¤±è´¥: {e}")
    
    return tuned_models, tuning_results

def save_models_and_results(trained_models, results, tuned_models, tuning_results):
    """ä¿å­˜æ¨¡å‹å’Œç»“æœ"""
    from pathlib import Path
    import json
    
    # åˆ›å»ºç›®å½•
    models_dir = Path(__file__).parent.parent / "outputs" / "models"
    tables_dir = Path(__file__).parent.parent / "outputs" / "tables"
    
    models_dir.mkdir(parents=True, exist_ok=True)
    tables_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. ä¿å­˜æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    print("\n ä¿å­˜æ¨¡å‹...")
    for name, model in trained_models.items():
        try:
            model_path = models_dir / f'{name.replace(" ", "_")}.pkl'
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            print(f"   {name}: å·²ä¿å­˜")
        except Exception as e:
            print(f"   ä¿å­˜{name}å¤±è´¥: {e}")
    
    # ä¿å­˜è°ƒä¼˜åçš„æ¨¡å‹
    for name, model in tuned_models.items():
        try:
            model_path = models_dir / f'{name.replace(" ", "_")}_tuned.pkl'
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            print(f"   {name}(è°ƒä¼˜å): å·²ä¿å­˜")
        except Exception as e:
            print(f"   ä¿å­˜{name}(è°ƒä¼˜å)å¤±è´¥: {e}")
    
    # 2. ä¿å­˜ç»“æœåˆ°CSV
    print("\n ä¿å­˜ç»“æœ...")
    
    # åŸºç¡€æ¨¡å‹ç»“æœ
    results_df = pd.DataFrame(results).T
    results_df = results_df.sort_values('F1åˆ†æ•°', ascending=False)
    results_df.to_csv(tables_dir / 'model_performance_comparison.csv')
    print(f"  âœ… æ¨¡å‹æ€§èƒ½æ¯”è¾ƒè¡¨å·²ä¿å­˜")
    
    # è°ƒä¼˜ç»“æœ
    if tuning_results:
        tuning_df = pd.DataFrame(tuning_results).T
        tuning_df.to_csv(tables_dir / 'hyperparameter_tuning_results.csv')
        print(f"   è¶…å‚æ•°è°ƒä¼˜ç»“æœå·²ä¿å­˜")
    
    # 3. ä¿å­˜ç»“æœä¸ºJSONï¼ˆä¾¿äºæŠ¥å‘Šä½¿ç”¨ï¼‰
    results_dict = {
        'åŸºç¡€æ¨¡å‹æ€§èƒ½': results_df.to_dict(),
        'è¶…å‚æ•°è°ƒä¼˜ç»“æœ': tuning_results
    }
    
    with open(tables_dir / 'model_results.json', 'w') as f:
        json.dump(results_dict, f, indent=2, default=str)
    
    print(f"   å®Œæ•´ç»“æœå·²ä¿å­˜ä¸ºJSONæ ¼å¼")
    
    return results_df

def identify_best_model(results_df, trained_models, tuned_models):
    """ç¡®å®šæœ€ä½³æ¨¡å‹"""
    print("\n" + "="*60)
    print(" ç¡®å®šæœ€ä½³æ¨¡å‹")
    print("="*60)
    
    # æ‰¾å‡ºåŸºç¡€æ¨¡å‹ä¸­F1åˆ†æ•°æœ€é«˜çš„
    best_basic_model = results_df.index[0]
    best_basic_score = results_df.iloc[0]['F1åˆ†æ•°']
    
    print(f"åŸºç¡€æ¨¡å‹ä¸­æœ€ä½³: {best_basic_model}")
    print(f"F1åˆ†æ•°: {best_basic_score:.4f}")
    
    # å¦‚æœæœ‰è°ƒä¼˜æ¨¡å‹ï¼Œæ¯”è¾ƒè°ƒä¼˜åçš„æ€§èƒ½
    if tuned_models:
        print(f"\nè°ƒä¼˜åçš„æ¨¡å‹:")
        for name, model in tuned_models.items():
            # éœ€è¦é‡æ–°è¯„ä¼°è°ƒä¼˜æ¨¡å‹
            from sklearn.metrics import f1_score
            
            # è¿™é‡Œéœ€è¦X_testå’Œy_testï¼Œæš‚æ—¶è·³è¿‡å…·ä½“è¯„ä¼°
            print(f"  {name}: å·²è°ƒä¼˜ï¼Œå‚æ•°å·²ä¿å­˜")
    
    # æ¨èæœ€ä½³æ¨¡å‹
    print(f"\n æ¨èæ¨¡å‹: {best_basic_model}")
    print(f"ç†ç”±: åœ¨åŸºç¡€æ¨¡å‹ä¸­F1åˆ†æ•°æœ€é«˜")
    
    return best_basic_model

def feature_importance_analysis_for_best_model(best_model_name, trained_models, X_train, feature_cols):
    """åˆ†ææœ€ä½³æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§"""
    print("\n" + "="*60)
    print(" æœ€ä½³æ¨¡å‹ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("="*60)
    
    model = trained_models.get(best_model_name)
    
    if model is None:
        print(f"æ‰¾ä¸åˆ°æ¨¡å‹: {best_model_name}")
        return None
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ç‰¹å¾é‡è¦æ€§å±æ€§
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        importance_df = pd.DataFrame({
            'ç‰¹å¾': feature_cols,
            'é‡è¦æ€§': importances
        }).sort_values('é‡è¦æ€§', ascending=False)
        
        print(f"å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾:")
        print(importance_df.head(10).to_string())
        
        # ä¿å­˜ç»“æœ
        from pathlib import Path
        tables_dir = Path(__file__).parent.parent / "outputs" / "tables"
        importance_df.to_csv(tables_dir / f'{best_model_name.replace(" ", "_")}_feature_importance.csv', index=False)
        
        print(f"\n ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜")
        
        return importance_df
    
    elif hasattr(model, 'coef_'):  # é€»è¾‘å›å½’ç­‰çº¿æ€§æ¨¡å‹
        coefficients = model.coef_[0]
        
        # åˆ›å»ºç³»æ•°DataFrame
        coef_df = pd.DataFrame({
            'ç‰¹å¾': feature_cols,
            'ç³»æ•°': coefficients,
            'ç³»æ•°ç»å¯¹å€¼': np.abs(coefficients)
        }).sort_values('ç³»æ•°ç»å¯¹å€¼', ascending=False)
        
        print(f"å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼ˆæŒ‰ç³»æ•°ç»å¯¹å€¼ï¼‰:")
        print(coef_df.head(10).to_string())
        
        # ä¿å­˜ç»“æœ
        from pathlib import Path
        tables_dir = Path(__file__).parent.parent / "outputs" / "tables"
        coef_df.to_csv(tables_dir / f'{best_model_name.replace(" ", "_")}_coefficients.csv', index=False)
        
        print(f"\n æ¨¡å‹ç³»æ•°å·²ä¿å­˜")
        
        return coef_df
    
    else:
        print(f"æ¨¡å‹ {best_model_name} æ²¡æœ‰ç‰¹å¾é‡è¦æ€§æˆ–ç³»æ•°å±æ€§")
        return None

def main_model_building():
    """ä¸»æ¨¡å‹å»ºç«‹æµç¨‹"""
    print("="*60)
    print(" ICUæ­»äº¡ç‡é¢„æµ‹æ¨¡å‹å»ºç«‹")
    print("="*60)
    
    # 1. åŠ è½½æ•°æ®
    print("\n æ­¥éª¤1: åŠ è½½æ•°æ®")
    data = load_training_data()
    if data[0] is None:
        return
    
    X_train, y_train, X_test, y_test, feature_cols = data
    
    # 2. ç‰¹å¾æ ‡å‡†åŒ–
    print("\n æ­¥éª¤2: ç‰¹å¾æ ‡å‡†åŒ–")
    X_train_scaled, X_test_scaled = scale_features(X_train, X_test)
    
    # 3. è®­ç»ƒå’Œè¯„ä¼°å¤šç§æ¨¡å‹
    print("\n æ­¥éª¤3: è®­ç»ƒå¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹")
    results, trained_models = train_and_evaluate_all_models(X_train_scaled, y_train, X_test_scaled, y_test)
    
    # 4. è¶…å‚æ•°è°ƒä¼˜
    print("\n æ­¥éª¤4: è¶…å‚æ•°è°ƒä¼˜")
    tuned_models, tuning_results = hyperparameter_tuning(X_train_scaled, y_train)
    
    # 5. ä¿å­˜æ¨¡å‹å’Œç»“æœ
    print("\n æ­¥éª¤5: ä¿å­˜æ¨¡å‹å’Œç»“æœ")
    results_df = save_models_and_results(trained_models, results, tuned_models, tuning_results)
    
    # 6. ç¡®å®šæœ€ä½³æ¨¡å‹
    print("\n æ­¥éª¤6: ç¡®å®šæœ€ä½³æ¨¡å‹")
    best_model = identify_best_model(results_df, trained_models, tuned_models)
    
    # 7. åˆ†ææœ€ä½³æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
    print("\n æ­¥éª¤7: åˆ†ææœ€ä½³æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§")
    feature_importance = feature_importance_analysis_for_best_model(
        best_model, trained_models, X_train_scaled, feature_cols
    )
    
    # 8. ç”Ÿæˆæ¨¡å‹å»ºç«‹æŠ¥å‘Š
    print("\n" + "="*60)
    print("æ¨¡å‹å»ºç«‹å®Œæˆï¼")
    print("="*60)
    
    print(f"\n æ¨¡å‹æ€§èƒ½æ€»ç»“:")
    print(results_df[['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'AUC']].head().to_string())
    
    print(f"\n ä¸‹ä¸€æ­¥:")
    print(f"  1. æŸ¥çœ‹ outputs/tables/model_performance_comparison.csv")
    print(f"  2. æŸ¥çœ‹ outputs/models/ ä¸­çš„ä¿å­˜çš„æ¨¡å‹")
    print(f"  3. è¿è¡Œ 05_model_evaluation.py è¿›è¡Œè¯¦ç»†è¯„ä¼°å’Œå¯è§†åŒ–")
    
    return best_model, results_df, trained_models

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    best_model, results_df, trained_models = main_model_building()